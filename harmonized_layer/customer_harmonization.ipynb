{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa9f61b-8925-454b-922f-aee186584475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCD Type 2 logic successfully executed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CustomerHarmonization\").getOrCreate()\n",
    "\n",
    "# Configuration\n",
    "source_table = \"processing_catalog.schema_raw_dimension.t_customers\"\n",
    "target_table = \"processing_catalog.schema_harmonized_dimension.t_customers\"\n",
    "job_id = \"batch_job2\"  # Replace with actual job ID\n",
    "\n",
    "# Step 1: Check if the harmonized table exists\n",
    "def check_table_exists():\n",
    "    query = f\"\"\"\n",
    "        SELECT count(1)\n",
    "        FROM processing_catalog.information_schema.tables\n",
    "        WHERE table_catalog = 'processing_catalog'\n",
    "        AND table_schema = 'schema_harmonized_dimension'\n",
    "        AND table_name = 't_customers'\n",
    "    \"\"\"\n",
    "    result = spark.sql(query).collect()[0][0]\n",
    "    return result > 0\n",
    "\n",
    "if check_table_exists():\n",
    "    latest_batch_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {source_table}\n",
    "        WHERE r_insert_timestamp > (\n",
    "            SELECT COALESCE(MAX(batch_r_timestamp), '1900-01-01')\n",
    "            FROM {target_table}\n",
    "        )\n",
    "    \"\"\"\n",
    "    latest_batch = spark.sql(latest_batch_query)\n",
    "    latest_batch.createOrReplaceTempView(\"latest_batch\")\n",
    "\n",
    "    # Step 3: Identify changed records\n",
    "    changed_records_query = f\"\"\"\n",
    "        SELECT\n",
    "            raw.customer_id,\n",
    "            raw.first_name,\n",
    "            raw.last_name,\n",
    "            raw.email,\n",
    "            raw.phone_number,\n",
    "            CAST(raw.loyalty_points AS INT) AS loyalty_points,\n",
    "            raw.membership_status,\n",
    "            raw.address,\n",
    "            raw.r_insert_timestamp AS batch_r_timestamp,\n",
    "            current_date() AS current_date,\n",
    "            current_timestamp() AS current_timestamp,\n",
    "            '{job_id}' AS job_id\n",
    "        FROM latest_batch raw\n",
    "        LEFT JOIN {target_table} harmonized\n",
    "        ON raw.customer_id = harmonized.customer_id\n",
    "        WHERE (\n",
    "            harmonized.customer_id IS NULL -- New customer\n",
    "            OR raw.first_name != harmonized.first_name\n",
    "            OR raw.last_name != harmonized.last_name\n",
    "            OR raw.email != harmonized.email\n",
    "            OR raw.phone_number != harmonized.phone_number\n",
    "            OR CAST(raw.loyalty_points AS INT) != harmonized.loyalty_points\n",
    "            OR raw.membership_status != harmonized.membership_status\n",
    "            OR raw.address != harmonized.address\n",
    "        )\n",
    "        AND (harmonized.active_flag = true OR harmonized.active_flag IS NULL) -- Only check current records\n",
    "    \"\"\"\n",
    "    changed_records = spark.sql(changed_records_query)\n",
    "    changed_records.createOrReplaceTempView(\"changed_records\")\n",
    "\n",
    "    # Step 4: Close current records in the harmonized table\n",
    "    update_existing_records_query = f\"\"\"\n",
    "        MERGE INTO {target_table} AS target\n",
    "        USING changed_records AS source\n",
    "        ON target.customer_id = source.customer_id\n",
    "        WHEN MATCHED AND target.active_flag = true THEN\n",
    "        UPDATE SET\n",
    "            target.active_flag = false,\n",
    "            target.end_effective_date = current_date(),\n",
    "            target.h_update_timestamp = current_timestamp()\n",
    "    \"\"\"\n",
    "    spark.sql(update_existing_records_query)\n",
    "\n",
    "    # Step 5: Insert new records into the harmonized table\n",
    "    insert_new_records_query = f\"\"\"\n",
    "        INSERT INTO {target_table} (\n",
    "            customer_id,\n",
    "            first_name,\n",
    "            last_name,\n",
    "            email,\n",
    "            phone_number,\n",
    "            loyalty_points,\n",
    "            membership_status,\n",
    "            address,\n",
    "            active_flag,\n",
    "            begin_effective_date,\n",
    "            end_effective_date,\n",
    "            batch_r_timestamp,\n",
    "            h_insert_timestamp,\n",
    "            h_update_timestamp,\n",
    "            job_id\n",
    "        )\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            first_name,\n",
    "            last_name,\n",
    "            email,\n",
    "            phone_number,\n",
    "            loyalty_points,\n",
    "            membership_status,\n",
    "            address,\n",
    "            true AS active_flag,\n",
    "            current_date() AS begin_effective_date,\n",
    "            DATE('9999-12-31') AS end_effective_date,\n",
    "            batch_r_timestamp,\n",
    "            current_timestamp() AS h_insert_timestamp,\n",
    "            current_timestamp() AS h_update_timestamp,\n",
    "            job_id\n",
    "        FROM changed_records\n",
    "    \"\"\"\n",
    "    spark.sql(insert_new_records_query)\n",
    "\n",
    "    print(\"SCD Type 2 logic successfully executed.\")\n",
    "else:\n",
    "    print(f\"Harmonized table '{target_table}' does not exist. Skipping processing.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "687a2076-9502-4f4e-9dd3-7d7d04fcca1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "customer_harmonization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}